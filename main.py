import os
import requests
from bs4 import BeautifulSoup
from supabase import create_client
from dotenv import load_dotenv
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service
import time
import re

load_dotenv()

SUPABASE_URL = os.environ["SUPABASE_URL"]
SUPABASE_KEY = os.environ["SUPABASE_SECRET_KEY"]

supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

LIST_URL = "https://crowdworks.jp/public/jobs"

def clean_text(text):
    return re.sub(r"\s+", " ", text).strip()

options = Options()
options.add_argument("--headless")

service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service, options=options)

print("ğŸ“¡ Selenium ãŒã‚¯ãƒ©ã‚¦ãƒ‰ãƒ¯ãƒ¼ã‚¯ã‚¹ä¸€è¦§ãƒšãƒ¼ã‚¸ã‚’é–‹ã„ã¦ã„ã¾ã™...")
driver.get(LIST_URL)

print("âŒ› JS ã®æç”»ãŒçµ‚ã‚ã‚‹ã®ã‚’å¾…ã£ã¦ã„ã¾ã™ï¼ˆ4ç§’ï¼‰...")
time.sleep(4)

print("ğŸ§ª HTML ã‚’è§£æä¸­...")
soup = BeautifulSoup(driver.page_source, "html.parser")

driver.quit()

# æ¡ˆä»¶ã‚«ãƒ¼ãƒ‰
jobs = soup.select("div._root_b2jur_2")
print("æ¡ˆä»¶æ•°:", len(jobs))

def extract_price(job):
    # é‡‘é¡ãƒœãƒƒã‚¯ã‚¹ã® div ã‚’æ¢ã™ï¼ˆclass åã®ä¸€éƒ¨ä¸€è‡´ã§æ¤œç´¢ï¼‰
    price_div = job.find("div", class_=lambda x: x and "_paymentBox" in x)
    if not price_div:
        return ""

    # å›ºå®šå ±é…¬ãƒ»æ™‚çµ¦ãªã©ã®ãƒ©ãƒ™ãƒ«
    label_tag = price_div.find("span", class_=lambda x: x and "paymentLabelPc" in x)
    label = label_tag.get_text(strip=True) if label_tag else ""

    # é‡‘é¡
    prices = price_div.find_all("span", class_=lambda x: x and "amountValuePc" in x)

    if len(prices) == 2:
        p1 = prices[0].get_text(strip=True)
        p2 = prices[1].get_text(strip=True)
        return f"{label}: {p1}ã€œ{p2}å††"

    if len(prices) == 1:
        p1 = prices[0].get_text(strip=True)
        return f"{label}: {p1}å††"

    return ""

# ğŸ”¹ è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰ã€Œä»•äº‹å†…å®¹æœ¬æ–‡ã€ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹é–¢æ•°
def scrape_detail_page(url: str) -> dict:
    """æ¡ˆä»¶ã®è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æœ¬æ–‡ãƒ»ä¼æ¥­åãƒ»äºˆç®—ãƒ»æ—¥ä»˜ã‚’ã¾ã¨ã‚ã¦å–å¾—ã™ã‚‹"""

    if not url:
        # URL ãŒç©ºãªã‚‰ç©ºãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
        return {
            "description": "",
            "employer": "",
            "budget": "",
            "posted_date": "",
            "start_date": "",
            "end_date": "",
        }

    headers = {
        "User-Agent": "Mozilla/5.0"
    }

    try:
        resp = requests.get(url, headers=headers, timeout=10)
        resp.raise_for_status()
    except requests.RequestException as e:
        print(f"[WARN] è©³ç´°ãƒšãƒ¼ã‚¸å–å¾—å¤±æ•—: {url} ({e})")
        return {
            "description": "",
            "employer": "",
            "budget": "",
            "posted_date": "",
            "start_date": "",
            "end_date": "",
        }

    detail_soup = BeautifulSoup(resp.text, "html.parser")

    # â‘  æœ¬æ–‡ï¼ˆtd.confirm_outside_link å†…ã®ãƒ†ã‚­ã‚¹ãƒˆï¼‰
    desc_td = detail_soup.select_one("td.confirm_outside_link")
    if desc_td:
        description = desc_td.get_text(strip=True, separator="\n")
    else:
        description = ""

    # â‘¡ ä¼æ¥­åï¼ˆä¾é ¼è€…åï¼‰
    employer_tag = detail_soup.select_one("a.display_link_none")
    employer = employer_tag.get_text(strip=True) if employer_tag else ""

    # â‘¢ äºˆç®—ï¼ˆå›ºå®šå ±é…¬åˆ¶ã®é‡‘é¡ãƒ¬ãƒ³ã‚¸ï¼‰
    budget_tag = detail_soup.select_one("div.fixed_price_budget")
    budget = budget_tag.get_text(strip=True) if budget_tag else ""

    # â‘£ æ²è¼‰æ—¥ãƒ»é–‹å§‹æ—¥ãƒ»çµ‚äº†æ—¥
    posted_date = ""
    start_date = ""
    end_date = ""

    # ã€Œè©³ç´°æƒ…å ±ã€ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰æ—¥ä»˜ã®è¡Œã‚’å–ã‚‹æƒ³å®š
    detail_table = detail_soup.select_one("section.cw-section.detail_information table.job_offer_detail_table")
    if detail_table:
        # 2è¡Œç›®ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹1ï¼‰ã® <td> ã‚’æƒ³å®š
        date_tds = detail_table.select("tbody tr:nth-of-type(2) td")
        if len(date_tds) >= 3:
            posted_date = date_tds[0].get_text(strip=True)
            start_date = date_tds[1].get_text(strip=True)
            end_date = date_tds[2].get_text(strip=True)

    return {
        "description": description,
        "employer": employer,
        "budget": budget,
        "posted_date": posted_date,
        "start_date": start_date,
        "end_date": end_date,
    }

def save_to_supabase(data: dict):
    """è¾æ›¸ãƒ‡ãƒ¼ã‚¿ã‚’ Supabase cw_jobs ãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜ã™ã‚‹"""

    # job_id ã¯ URL ã®æœ«å°¾ã®æ•°å­—ã‚’å–å¾—
    job_id = data["url"].rstrip("/").split("/")[-1]

    row = {
        "job_id": job_id,
        "title": data["title"],
        "url": data["url"],
        "category": data["category"],
        "price": data["price"],
        "employer": data["employer"],
        "budget_detail": data["budget_detail"],
        "posted_date": data["posted_date"],
        "start_date": data["start_date"],
        "end_date": data["end_date"],
        "description": data["description"],
    }

    # â˜… æ—¢å­˜ãƒã‚§ãƒƒã‚¯ã‚’ã—ã¦ upsert ã™ã‚‹ï¼ˆé‡è¤‡é˜²æ­¢ï¼‰
    supabase.table("cw_jobs").upsert(row, on_conflict="job_id").execute()

# ğŸ”„ ä¸€è¦§ã®å„æ¡ˆä»¶ã‚’ãƒ«ãƒ¼ãƒ—
for idx, job in enumerate(jobs, start=1):

    # ã‚¿ã‚¤ãƒˆãƒ«
    title_a = job.select_one("h3 a")
    title = title_a.get_text(strip=True) if title_a else ""

    # â˜… URL ã‚’æ­£ã—ãå–å¾—
    url = title_a.get("href") if title_a else ""

    # â˜… ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
    if url.startswith("/"):
        url = "https://crowdworks.jp" + url


    # ã‚«ãƒ†ã‚´ãƒªï¼ˆaã‚¿ã‚°ï¼‰
    category_div = job.select_one("div._jobCategoryVue_b2jur_52")
    if category_div:
        category_a = category_div.find("a")
        category = category_a.get_text(strip=True) if category_a else ""
    else:
        category = ""

    # é‡‘é¡
    price = extract_price(job)

    # â˜… è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æœ¬æ–‡å–å¾—
    detail = scrape_detail_page(url)
    description = detail["description"]
    employer = detail["employer"]
    budget_detail = detail["budget"]
    posted_date = detail["posted_date"]
    start_date = detail["start_date"]
    end_date = detail["end_date"]

    print(f"\n=== {idx} ä»¶ç›® ===")
    print("ã‚¿ã‚¤ãƒˆãƒ«:", title)
    print("URL:", url)
    print("ã‚«ãƒ†ã‚´ãƒª:", category)
    print("é‡‘é¡:", price)
    print("ä¾é ¼è€…:", employer)
    print("è©³ç´°äºˆç®—:", budget_detail)
    print("æ²è¼‰æ—¥:", posted_date)
    print("é–‹å§‹æ—¥:", start_date)
    print("çµ‚äº†æ—¥:", end_date)

    # æœ¬æ–‡ã¯é•·ã„ã®ã§å…ˆé ­ã ã‘ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
    if description:
        preview = description.replace("\n", " ")
        if len(preview) > 120:
            preview = preview[:120] + "..."
        print("æœ¬æ–‡æŠœç²‹:", preview)
    else:
        print("æœ¬æ–‡æŠœç²‹: å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")

    # ç›¸æ‰‹ã‚µã‚¤ãƒˆã¸ã®è² è·ã‚’ä¸‹ã’ã‚‹ãŸã‚ã€å°‘ã—å¾…ã¤
    time.sleep(1.5)

    # â˜… DB ã«ä¿å­˜
    save_to_supabase({
    "title": title,
    "url": url,
    "category": category,
    "price": price,
    "employer": employer,
    "budget_detail": budget_detail,
    "posted_date": posted_date,
    "start_date": start_date,
    "end_date": end_date,
    "description": description,  # â­å…¨æ–‡
})


